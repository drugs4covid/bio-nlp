{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cord19-indexing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPIbcPnjwXIJwHa3cf86xLh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/librairy/bio-nlp/blob/master/notebooks/documents/cord19_indexing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEhcuAIRIkyp"
      },
      "source": [
        "This notebook indexes the documents published by [CORD-19](https://www.semanticscholar.org/cord19) in the [Drugs4Covid](drugs4covid.oeg.fi.upm.es/) repository. Two levels of information are considered: documents and paragraphs. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5Lk6IJeJrZ9"
      },
      "source": [
        "First we need to import the libraries needed to access the repository in Solr, parse the JSON requests through its API and manage the operations in parallel to speed up the process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2o9psxnIIRHq"
      },
      "source": [
        "#!/usr/bin/env python3\n",
        "import tarfile\n",
        "import urllib.request\n",
        "import json\n",
        "import requests\n",
        "import pysolr\n",
        "import os\n",
        "import multiprocessing as mp\n",
        "from datetime import datetime\n",
        "import hashlib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_KqMhVqKWEN"
      },
      "source": [
        "We then register the access points. To the drugs4covid repository and to the service offered by SemanticScholar for linking scientific articles. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdDBI7aVIrYh"
      },
      "source": [
        "solr = pysolr.Solr('https://librairy.linkeddata.es/data/paragraphs', timeout=10)\n",
        "\n",
        "cord19_url = \"https://cord-19.apps.allenai.org/paper/\"\n",
        "unknown_section = \"(which was not peer-reviewed)\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1UY5r2fLXmu"
      },
      "source": [
        "In our Solr repository each document contains a unique identifier automatically generated from its content (`id`), the identifier given by SemanticScholar to link it to that repository (`article_id_s`), the section it belongs to (`section_s`), the full text it contains (`text_t`) and its length (`size_i`). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzHi-IoWIuoC"
      },
      "source": [
        "def get_document(id,json_paragraph):\n",
        "    document = {}\n",
        "    section = json_paragraph['section'].lower()\n",
        "    if (section == unknown_section or len(section) == 0):\n",
        "        section = \"body\"\n",
        "    document['section_s'] = section\n",
        "    text = json_paragraph['text']\n",
        "    document['text_t'] = text\n",
        "    text_content = id + text + section\n",
        "    hash_object = hashlib.md5(text_content.encode())\n",
        "    document['id'] = hash_object.hexdigest()\n",
        "    document['article_id_s'] = id\n",
        "    document['size_i'] = len(text)\n",
        "    return document"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxwvVWabLffa"
      },
      "source": [
        "All this information is extracted from the CORD-19 corpus for each document and paragraph and stored in the Drugs4Covid repository.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OxNCiG2Iwh3"
      },
      "source": [
        "def get_documents(file_url):\n",
        "    with open(file_url) as f:\n",
        "      license = file_url.split(\"/\")[4]\n",
        "      data = json.load(f)\n",
        "      documents = []\n",
        "      id = data['paper_id']\n",
        "      results = solr.search(\"id:\"+id)\n",
        "      if (len(results) > 0):\n",
        "            print(\"Found\",document[\"id\"])\n",
        "            return results\n",
        "      if ('abstract' in data):\n",
        "          for abstract in data['abstract']:\n",
        "              documents.append(get_document(id,abstract))\n",
        "      if ('body_text' in data):\n",
        "          for paragraph in data['body_text']:\n",
        "              documents.append(get_document(id,paragraph))\n",
        "      #print(len(documents), \"paragraphs retrieved from paper: \", id)\n",
        "      return documents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_J_nYGYLj9e"
      },
      "source": [
        "Datasets must be downloaded from the [CORD-19 download section](https://ai2-semanticscholar-cord-19.s3-us-west-2.amazonaws.com/historical_releases.html) and stored locally in order to be processed. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lp7gLqnsIy1V"
      },
      "source": [
        "# Articles\n",
        "directories = [\n",
        "    (\"/Users/cbadenes/Downloads/covid19/custom_license/pdf_json\",\"custom_license\"),\n",
        "    (\"/Users/cbadenes/Downloads/covid19/custom_license/pmc_json\",\"custom_license\"),\n",
        "    (\"/Users/cbadenes/Downloads/covid19/comm_use_subset/pmc_json\",\"commercial_use\"),\n",
        "    (\"/Users/cbadenes/Downloads/covid19/comm_use_subset/pdf_json\",\"commercial_use\"),\n",
        "    (\"/Users/cbadenes/Downloads/covid19/biorxiv_medrxiv/pdf_json\",\"biorxiv\"),\n",
        "    (\"/Users/cbadenes/Downloads/covid19/noncomm_use_subset/pmc_json\",\"noncommercial_use\"),\n",
        "    (\"/Users/cbadenes/Downloads/covid19/noncomm_use_subset/pdf_json\",\"noncommercial_use\")\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2lSFr3XMRr9"
      },
      "source": [
        "Once the datasets are available and the actions to parse their content are implemented, we distribute the processing tasks locally among several processors to speed up the process. \n",
        "\n",
        "We use the [multiprocessing library](https://docs.python.org/3/library/multiprocessing.html) to parallelise the tasks. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9rOIgVxI2nj"
      },
      "source": [
        "pool = mp.Pool(4)\n",
        "\n",
        "for directory in directories:\n",
        "    print(\"Indexing directory\", directory)\n",
        "    directory_path = directory[0]\n",
        "    files = os.listdir(directory_path)\n",
        "    min = 0\n",
        "    max = 0\n",
        "    incr = 500\n",
        "    counter = 0\n",
        "    while(max < len(files)):\n",
        "        min = counter\n",
        "        max = min + incr\n",
        "        if (max > len(files)):\n",
        "            max = len(files)\n",
        "        documents = pool.map(get_documents, [directory_path + \"/\" + file for file in files[min:max]])\n",
        "        commit_documents = [paragraph for paragraphs in documents for paragraph in paragraphs]\n",
        "        print(\"[\",datetime.now(),\"]\",\"indexing\",len(commit_documents),\" docs...\")\n",
        "        try:\n",
        "            solr.add(commit_documents)\n",
        "            solr.commit()\n",
        "        except:\n",
        "            print(\"Solr query error. Wait for 5secs..\")\n",
        "            time.sleep(5.0)\n",
        "            solr.commit()\n",
        "        counter=max\n",
        "\n",
        "print(counter,\"docs added\")\n",
        "pool.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}